{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    AIMessagePromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baca9619066ac3df",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "**Note:** Worked with Chanteria Milner on this assignment.\n",
        "<img src=\"misc/syllabus_segment.png\" style=\"width:400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc1aa3b4deec182",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Constants, Utility Functions, and Data Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ef1a145a95feae",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Constants and clients\n",
        "GPT_MODEL = \"gpt-3.5-turbo\"\n",
        "MAX_CHAR_LEN = 5000000\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbc9341a70ed576",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "\n",
        "\n",
        "def kl_divergence(x, y):\n",
        "    P = x.copy()\n",
        "    Q = y.copy()\n",
        "    P.columns = [\"P\"]\n",
        "    Q.columns = [\"Q\"]\n",
        "    df = Q.join(P).fillna(0)\n",
        "    p = df.iloc[:, 1]\n",
        "    q = df.iloc[:, 0]\n",
        "    D_kl = scipy.stats.entropy(p, q)\n",
        "    return D_kl\n",
        "\n",
        "\n",
        "def chi2_divergence(x, y):\n",
        "    P = x.copy()\n",
        "    Q = y.copy()\n",
        "    P.columns = [\"P\"]\n",
        "    Q.columns = [\"Q\"]\n",
        "    df = Q.join(P).fillna(0)\n",
        "    p = df.iloc[:, 1]\n",
        "    q = df.iloc[:, 0]\n",
        "    return scipy.stats.chisquare(p, q).statistic\n",
        "\n",
        "\n",
        "def corpus_divergence(corpus1, corpus2, difference=\"KL\"):\n",
        "    \"\"\"Difference parameter can equal KL, Chi2, or Wass\"\"\"\n",
        "    freqP = nltk.FreqDist(corpus1)\n",
        "    P = pd.DataFrame(\n",
        "        list(freqP.values()), columns=[\"frequency\"], index=list(freqP.keys())\n",
        "    )\n",
        "    freqQ = nltk.FreqDist(corpus2)\n",
        "    Q = pd.DataFrame(\n",
        "        list(freqQ.values()), columns=[\"frequency\"], index=list(freqQ.keys())\n",
        "    )\n",
        "    if difference == \"KL\":\n",
        "        return kl_divergence(P, Q)\n",
        "    elif difference == \"Chi2\":\n",
        "        return chi2_divergence(P, Q)\n",
        "    elif difference == \"KS\":\n",
        "        try:\n",
        "            return scipy.stats.ks_2samp(P[\"frequency\"], Q[\"frequency\"]).statistic\n",
        "        except:\n",
        "            return scipy.stats.ks_2samp(P[\"frequency\"], Q[\"frequency\"])\n",
        "    elif difference == \"Wasserstein\":\n",
        "        try:\n",
        "            return scipy.stats.wasserstein_distance(\n",
        "                P[\"frequency\"], Q[\"frequency\"], u_weights=None, v_weights=None\n",
        "            ).statistic\n",
        "        except:\n",
        "            return scipy.stats.wasserstein_distance(\n",
        "                P[\"frequency\"], Q[\"frequency\"], u_weights=None, v_weights=None\n",
        "            )\n",
        "\n",
        "\n",
        "def get_density(df):\n",
        "    data = df\n",
        "    density = scipy.stats.gaussian_kde(data)\n",
        "    width = np.max(data) - np.min(data)\n",
        "    xs = np.linspace(np.min(data) - width / 5, np.max(data) + width / 5, 600)\n",
        "    density.covariance_factor = lambda: 0.25\n",
        "    density._compute_covariance()\n",
        "    return xs, density(xs)\n",
        "\n",
        "\n",
        "def draw_network(df, title):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    G = nx.DiGraph()\n",
        "    for from_ in df.index:\n",
        "        for to_ in df.columns:\n",
        "            G.add_edge(from_, to_, weight=df.loc[from_][to_])\n",
        "\n",
        "    pos = nx.spring_layout(G, k=0.55, iterations=20)\n",
        "    edges, weights = zip(*nx.get_edge_attributes(G, \"weight\").items())\n",
        "    weights = np.array(weights)\n",
        "    # weights = weights*weights\n",
        "    weights = 6 * weights / np.max(weights)\n",
        "    print(title)\n",
        "\n",
        "    edge_colors = 20 * (weights / np.max(weights))\n",
        "    edge_colors = edge_colors.astype(int)\n",
        "    #     nx.draw_networkx_nodes(G,pos,node_size=1200,alpha=0.7,node_color='#99cef7')\n",
        "    #     nx.draw_networkx_edges(G,pos,edge_color=edge_colors)\n",
        "    #     nx.draw_networkx_labels(G,pos,font_weight='bold')\n",
        "    nx.draw(\n",
        "        G,\n",
        "        pos,\n",
        "        with_labels=True,\n",
        "        font_weight=\"bold\",\n",
        "        width=weights,\n",
        "        edge_color=255 - edge_colors,\n",
        "        node_color=\"#99cef7\",\n",
        "        node_size=1200,\n",
        "        alpha=0.75,\n",
        "        arrows=True,\n",
        "        arrowsize=20,\n",
        "    )\n",
        "    return edge_colors\n",
        "\n",
        "\n",
        "def create_system_message_prompt():\n",
        "    \"\"\"Creates a system message prompt\"\"\"\n",
        "    personality_template = \"\"\"\n",
        "    The following is a conversation with an AI assistant.\n",
        "    \"\"\"\n",
        "    return SystemMessagePromptTemplate.from_template(personality_template)\n",
        "\n",
        "\n",
        "def create_chat_prompt(human_history, ai_history):\n",
        "    \"\"\"Creates a chat prompt template with human history, and AI history.\"\"\"\n",
        "    messages = []\n",
        "    create_system_message_prompt()\n",
        "\n",
        "    for h, a in zip(human_history, ai_history):\n",
        "        messages.append(HumanMessagePromptTemplate.from_template(h))\n",
        "        messages.append(AIMessagePromptTemplate.from_template(a))\n",
        "\n",
        "    messages.append(HumanMessagePromptTemplate.from_template(\"{input}\"))\n",
        "    return ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "\n",
        "def query_chain(chain, input_text):\n",
        "    \"\"\"Queries the conversation chain with the given input.\"\"\"\n",
        "    return chain.run(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9f4f4a6704a9d6",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Data importing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d64bc92c9f05ea22",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 1*</font>\n",
        "\n",
        "<font color=\"red\">Construct cells immediately below this that use ConvoKit to analyze a Corpus other \n",
        "than 'subreddit-Cornell', including at least one function you find in the package \n",
        "not used above. You can also generate a ConvoKit Corpus from your own dataset based \n",
        "on [their Corpus from .txt files tutorial](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/converting_movie_corpus.ipynb) or [their Corpus from pandas tutorial](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/corpus_from_pandas.ipynb), but that may \n",
        "be time-consuming for a weekly assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db52d201f66fc01b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-25T22:11:17.590522Z",
          "start_time": "2024-02-25T22:11:17.585897Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "908dc21f632ad37e",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 2*</font>\n",
        "\n",
        "<font color=\"red\">Construct cells immediately below this that perform a similar social \n",
        "similarity or influence analysis on a dataset relevant to your final project (__or \n",
        "one from ConvoKit__). Create relationships between actors in a network based on your \n",
        "dataset (e.g., person to person or document to document), and perform analyses that \n",
        "interrogate the structure of their interactions, similarity, and/or influence on \n",
        "one another. (For example, if relevant to your final project, you could explore \n",
        "different soap operas, counting how many times a character may have used the word \n",
        "love in conversation with another character, and identify if characters in love \n",
        "speak like each other. Or do opposites attract?) What does that analysis and its \n",
        "output reveal about the relative influence of each actor on others? What does it \n",
        "reveal about the social game being played?\n",
        "\n",
        "<font color=\"red\">Stretch 1:\n",
        "Render the social network with weights (e.g., based on the number of scenes in \n",
        "which actors appear together), then calculate the most central actors in the \n",
        "`show.Realtime` output can be viewed in shell.\n",
        "\n",
        "<font color=\"red\">Stretch 2:\n",
        "Implement more complex measures of similarity based on the papers you have read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0d9307c9a390ff",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-25T22:11:17.590825Z",
          "start_time": "2024-02-25T22:11:17.589715Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "670ab4047643c5a7",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 3*</font>\n",
        "\n",
        "<font color=\"red\">Review the documentation for tools and agents from LangChain. Use at \n",
        "least two tools with appropriate agents discovered during your review to construct a \n",
        "chain addressing questions pertinent to your final project. If your project dataset \n",
        "is unsuitable for this task, select an alternative small-sized dataset for \n",
        "implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79f2959169df6d40",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-25T22:11:17.751124Z",
          "start_time": "2024-02-25T22:11:17.732118Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ff78f725cfb57da5",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 4*</font>\n",
        "\n",
        "<font color=\"red\">Use LangChain(you're welcome to not use it) to set up conversations with LLM \n",
        "agents for questions related to your final project (if relevant), or think of a \n",
        "scenario that a simulated conversation could be useful to answer a research question \n",
        "and find a dataset to implement it. What does it reveal about the social game involved \n",
        "with your dataset?\n",
        "\n",
        "<font color=\"red\"> Stretch: Use the idea of memory retrieval(or other methods) to design better \n",
        "templates for the LLM conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79893be96979326",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
