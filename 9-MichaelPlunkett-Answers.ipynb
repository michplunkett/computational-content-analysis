{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "851b545bc0e65160",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Constants, Utility Functions, and Data Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb1952c0d22688d9",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Constants and clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8d130f3e4ba6a7a",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1a3a598d74b713",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Data importing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d36d8cdb7627b74c",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 1*</font>\n",
        "\n",
        "<font color=\"red\">Construct cells immediately below this that read in 10 audio files (e.g., produced on your smartphone recorder?) from at least two different speakers, which include sentences of different types (e.g., question, statement, exclamation). At least two of these should include recordings of the two speakers talking to each other (e.g., a simple question/answer). Contrast the frequency distributions of the words spoken within speaker. What speaker's voice has a higher and which has lower frequency? What words are spoken at the highest and lowest frequencies? What parts-of-speech tend to be high or low? How do different types of sentences vary in their frequency differently? When people are speaking to each other, how do their frequencies change? Whose changes more?\n",
        "    \n",
        "OR\n",
        "\n",
        "<font color=\"red\">Construct cells immediately below this that use the 10 audio files from at least two different speakers read in previously, attempt to automatically extract the words from Google, and calculate the word-error rate, as described in Chapter 9 from *Jurafsky & Martin*, page 334. How well does it do? Under what circumstances does it perform poorly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29614afde37378da",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fadecd172bec45a5",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 2*</font>\n",
        "\n",
        "<font color=\"red\"> We've seen two ways in which we can use the wav2vec2 model from Transformers - to generate a high dimensional embedding, and to transcribe an audio file into text. All of this is done using a Deep Neural model!\n",
        "\n",
        "<font color=\"red\"> In the following cells, use these methods to conduct analysis of the 10 audio files which you have - maybe add 2 songs to the mix as well. How do the embeddings cluster the audio files? Which files are most similar to each other? How does the model perform with respect to a file recorded by you versus a song, versus the sample dataset? How can you incorporate the wav2vec2 model and other multi-modal data into your projects?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7765688dd4beef37",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "95c80595adac68eb",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 3*</font>\n",
        "\n",
        "<font color=\"red\">You can either:\n",
        "    \n",
        "<font color=\"red\">a) Construct cells immediately below this that report the results from experiments in which you place each of images taken or retrieved for the last exercise through the online demos for [caffe](http://demo.caffe.berkeleyvision.org) and [places](http://places.csail.mit.edu/demo.html). Paste the image and the output for both object detector and scene classifier below, beside one another. Calculate precision and recall for caffe's ability to detect objects of interest across your images. What do you think about Places' scene categories and their assignments to your images?\n",
        "    \n",
        "<font color=\"red\">b) Implement any one deep learning example using PyTorch and images. What does the pre-trained model see in your images?\n",
        "    \n",
        "<font color=\"red\">c) Use some form of vectorisation of images - (a deep learning one, or an RGB representation, or HSV) and use similarity measures or clustering to explore your image data.\n",
        "\n",
        "<font color=\"red\">Could you use image classification to enhance your research project and, if so, how? How would multi-modal data sources make your analysis more powerful?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1d0df78cfe94e",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
