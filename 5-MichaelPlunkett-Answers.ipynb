{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:52:30.503112Z",
          "start_time": "2024-02-05T00:52:27.236073Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from functools import reduce\n",
        "from itertools import permutations\n",
        "\n",
        "import lucem_illud\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "import sklearn\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dbb91751449d0ac",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Constants, Utility Functions, and Data Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a85842706707fe63",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:45:10.544373Z",
          "start_time": "2024-02-05T00:45:10.282618Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d06bd1e4a8c2b26e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:45:10.689722Z",
          "start_time": "2024-02-05T00:45:10.292442Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Utility Functions\n",
        "\n",
        "\n",
        "def pairwise_metric_average(metric, array):\n",
        "    \"\"\"\n",
        "    Calculate the pairwise metric average for the real\n",
        "    elements of metric function run on an array of annotations.\n",
        "    \"\"\"\n",
        "    p = permutations(range(array[0, :].size), 2)\n",
        "    m = [metric(array[:, x[0]], array[:, x[1]]) for x in p]\n",
        "    clean_m = [c for c in m if not math.isnan(c)]\n",
        "    return reduce(lambda a, b: a + b, clean_m) / len(clean_m)\n",
        "\n",
        "\n",
        "def random(num_per_category=500):\n",
        "    datDict = {\n",
        "        \"vect\": [np.random.rand(2) * 2 - 1 for i in range(2 * num_per_category)],\n",
        "        \"category\": [i % 2 for i in range(2 * num_per_category)],\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(datDict)\n",
        "\n",
        "\n",
        "def and_split(noise=0, num_per_category=500):\n",
        "    def gen_point(cat):\n",
        "        y = np.random.random_sample() * 2 - 1\n",
        "        if noise >= 0:\n",
        "            x = (\n",
        "                np.random.random_sample()\n",
        "                - cat\n",
        "                - (np.random.random_sample() - cat) * noise\n",
        "            )\n",
        "        else:\n",
        "            x = (1 - noise * np.random.random_sample()) - cat\n",
        "        return np.array([x, y])\n",
        "\n",
        "    datDict = {\n",
        "        \"vect\": [gen_point(i % 2) for i in range(2 * num_per_category)],\n",
        "        \"category\": [i % 2 for i in range(2 * num_per_category)],\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(datDict)\n",
        "\n",
        "\n",
        "def xor_split(noise=0, num_per_category=500):\n",
        "    def gen_point(cat):\n",
        "        if cat == 1:\n",
        "            if np.random.randint(0, 2) < 1:\n",
        "                y = np.random.random_sample() - np.random.random_sample() * noise * 2\n",
        "                x = np.random.random_sample() - np.random.random_sample() * noise * 2\n",
        "            else:\n",
        "                y = (\n",
        "                    np.random.random_sample()\n",
        "                    - 1\n",
        "                    - np.random.random_sample() * noise * 2\n",
        "                )\n",
        "                x = (\n",
        "                    np.random.random_sample()\n",
        "                    - 1\n",
        "                    - np.random.random_sample() * noise * 2\n",
        "                )\n",
        "        else:\n",
        "            if np.random.randint(0, 2) < 1:\n",
        "                y = (\n",
        "                    np.random.random_sample()\n",
        "                    - 1\n",
        "                    - np.random.random_sample() * noise * 2\n",
        "                )\n",
        "                x = np.random.random_sample() - np.random.random_sample() * noise * 2\n",
        "            else:\n",
        "                y = np.random.random_sample() - np.random.random_sample() * noise * 2\n",
        "                x = (\n",
        "                    np.random.random_sample()\n",
        "                    - 1\n",
        "                    - np.random.random_sample() * noise * 2\n",
        "                )\n",
        "        return np.array([x, y])\n",
        "\n",
        "    datDict = {\n",
        "        \"vect\": [gen_point(i % 2) for i in range(2 * num_per_category)],\n",
        "        \"category\": [i % 2 for i in range(2 * num_per_category)],\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(datDict)\n",
        "\n",
        "\n",
        "def target_split(noise=0, num_per_category=500, inner_rad=0.3):\n",
        "    def gen_point(cat):\n",
        "        if cat == 0:\n",
        "            r = (\n",
        "                np.random.random_sample() * inner_rad\n",
        "                + (1 - inner_rad) * np.random.random_sample() * noise\n",
        "            )\n",
        "        else:\n",
        "            r = (\n",
        "                np.random.random_sample() * (1 - inner_rad)\n",
        "                + inner_rad\n",
        "                - inner_rad * np.random.random_sample() * noise\n",
        "            )\n",
        "        eta = 2 * np.pi * np.random.random_sample()\n",
        "        return np.array([r * np.cos(eta), r * np.sin(eta)])\n",
        "\n",
        "    datDict = {\n",
        "        \"vect\": [gen_point(i % 2) for i in range(2 * num_per_category)],\n",
        "        \"category\": [i % 2 for i in range(2 * num_per_category)],\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(datDict)\n",
        "\n",
        "\n",
        "def multi_blobs(noise=0, num_per_category=500, centers=5):\n",
        "    if isinstance(centers, int):\n",
        "        n_samples = num_per_category * centers\n",
        "    else:\n",
        "        n_samples = num_per_category * len(centers)\n",
        "    X, y = sklearn.datasets.make_blobs(\n",
        "        n_samples=n_samples, centers=centers, cluster_std=(0.8 * (noise * 2 + 1))\n",
        "    )\n",
        "    datDict = {\n",
        "        \"vect\": list(X),\n",
        "        \"category\": y,\n",
        "    }\n",
        "    return pd.DataFrame(datDict)\n",
        "\n",
        "\n",
        "def plotter(df: pd.DataFrame, category_key: str = \"category\"):\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    pallet = seaborn.color_palette(\n",
        "        palette=\"rainbow\", n_colors=len(set(df[category_key]))\n",
        "    )\n",
        "    for i, cat in enumerate(set(df[category_key])):\n",
        "        a = np.stack(df[df[category_key] == cat][\"vect\"])\n",
        "        ax.scatter(a[:, 0], a[:, 1], c=pallet[i], label=cat)\n",
        "    ax.legend(loc=\"center right\", title=\"Categories\")\n",
        "    ax.axis(\"off\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def pca_split_stats(\n",
        "    test_df: pd.DataFrame,\n",
        "    train_df: pd.DataFrame,\n",
        "    red_pca_key: str,\n",
        "    category_key: str = \"category\",\n",
        ") -> None:\n",
        "    logistic = sklearn.linear_model.LogisticRegression()\n",
        "    train_df[red_pca_key] = train_df[\"pca\"].apply(lambda x: x[:400])\n",
        "    test_df[red_pca_key] = test_df[\"pca\"].apply(lambda x: x[:400])\n",
        "\n",
        "    logistic.fit(np.stack(train_df[red_pca_key], axis=0), train_df[category_key])\n",
        "\n",
        "    print(\"Training:\")\n",
        "    print(\n",
        "        logistic.score(np.stack(train_df[red_pca_key], axis=0), train_df[category_key])\n",
        "    )\n",
        "    print(\"Testing:\")\n",
        "    print(logistic.score(np.stack(test_df[red_pca_key], axis=0), test_df[category_key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "aff1aca6913b63a4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:45:10.724709Z",
          "start_time": "2024-02-05T00:45:10.295116Z"
        },
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comments</th>\n      <th>disposition</th>\n      <th>incident</th>\n      <th>location</th>\n      <th>occurred</th>\n      <th>predicted_incident</th>\n      <th>reported</th>\n      <th>reported_date</th>\n      <th>ucpd_id</th>\n      <th>validated_address</th>\n      <th>validated_location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A person was transported to Comer Hospital by ...</td>\n      <td>Closed</td>\n      <td>Mental Health Transport</td>\n      <td>6300 S. University Ave. (S. Woodlawn Ave. Char...</td>\n      <td>2/1/24 10:10 AM</td>\n      <td>NaN</td>\n      <td>2024-02-01T10:10:00-06:00</td>\n      <td>2024-02-01</td>\n      <td>24-00114</td>\n      <td>6300 S UNIVERSITY AVE, CHICAGO, IL, 60637</td>\n      <td>41.78045407997166,-87.59732203570559</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Catalytic converter taken from a 2015 Toyota p...</td>\n      <td>Open</td>\n      <td>Theft from Motor Vehicle</td>\n      <td>1210 E. 57th St. (Public Way)</td>\n      <td>1/31/24 to 2/1/24 6:00 PM to 7:00 AM</td>\n      <td>NaN</td>\n      <td>2024-02-01T10:18:00-06:00</td>\n      <td>2024-02-01</td>\n      <td>24-00115</td>\n      <td>1210 E 57TH ST, CHICAGO, IL, 60637</td>\n      <td>41.79150658678615,-87.59602168542061</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Debit and credit cards taken from wallet in un...</td>\n      <td>Open</td>\n      <td>Theft</td>\n      <td>900 E. 57th St. (Knapp Center)</td>\n      <td>2/1/24 9:50 AM to 4:30 PM</td>\n      <td>NaN</td>\n      <td>2024-02-01T17:54:00-06:00</td>\n      <td>2024-02-01</td>\n      <td>24-00116</td>\n      <td>900 E 57TH ST, CHICAGO, IL, 60637</td>\n      <td>41.791423455510476,-87.60366291896175</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Boyfriend battered girlfriend in off-campus pr...</td>\n      <td>Open</td>\n      <td>Domestic Battery</td>\n      <td>6040 S. Harper Ave. (Apt. Building)</td>\n      <td>2/1/24 2:45 PM</td>\n      <td>NaN</td>\n      <td>2024-02-01T15:45:00-06:00</td>\n      <td>2024-02-01</td>\n      <td>24-00117</td>\n      <td>6040 S HARPER AVE, CHICAGO, IL, 60637</td>\n      <td>41.78472618578524,-87.58821677767634</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A known suspect entered the off-campus store a...</td>\n      <td>Referred</td>\n      <td>Information / Theft</td>\n      <td>1346 E. 53rd St. (Target)</td>\n      <td>1/31/24 12:15 PM</td>\n      <td>NaN</td>\n      <td>2024-01-31T14:16:00-06:00</td>\n      <td>2024-01-31</td>\n      <td>2024-004118</td>\n      <td>1346 E 53RD ST, CHICAGO, IL, 60615</td>\n      <td>41.79955044222366,-87.593062823983</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                            comments disposition  \\\n0  A person was transported to Comer Hospital by ...      Closed   \n1  Catalytic converter taken from a 2015 Toyota p...        Open   \n2  Debit and credit cards taken from wallet in un...        Open   \n3  Boyfriend battered girlfriend in off-campus pr...        Open   \n4  A known suspect entered the off-campus store a...    Referred   \n\n                   incident  \\\n0   Mental Health Transport   \n1  Theft from Motor Vehicle   \n2                     Theft   \n3          Domestic Battery   \n4       Information / Theft   \n\n                                            location  \\\n0  6300 S. University Ave. (S. Woodlawn Ave. Char...   \n1                      1210 E. 57th St. (Public Way)   \n2                     900 E. 57th St. (Knapp Center)   \n3                6040 S. Harper Ave. (Apt. Building)   \n4                          1346 E. 53rd St. (Target)   \n\n                               occurred predicted_incident  \\\n0                       2/1/24 10:10 AM                NaN   \n1  1/31/24 to 2/1/24 6:00 PM to 7:00 AM                NaN   \n2             2/1/24 9:50 AM to 4:30 PM                NaN   \n3                        2/1/24 2:45 PM                NaN   \n4                      1/31/24 12:15 PM                NaN   \n\n                    reported reported_date      ucpd_id  \\\n0  2024-02-01T10:10:00-06:00    2024-02-01     24-00114   \n1  2024-02-01T10:18:00-06:00    2024-02-01     24-00115   \n2  2024-02-01T17:54:00-06:00    2024-02-01     24-00116   \n3  2024-02-01T15:45:00-06:00    2024-02-01     24-00117   \n4  2024-01-31T14:16:00-06:00    2024-01-31  2024-004118   \n\n                           validated_address  \\\n0  6300 S UNIVERSITY AVE, CHICAGO, IL, 60637   \n1         1210 E 57TH ST, CHICAGO, IL, 60637   \n2          900 E 57TH ST, CHICAGO, IL, 60637   \n3      6040 S HARPER AVE, CHICAGO, IL, 60637   \n4         1346 E 53RD ST, CHICAGO, IL, 60615   \n\n                      validated_location  \n0   41.78045407997166,-87.59732203570559  \n1   41.79150658678615,-87.59602168542061  \n2  41.791423455510476,-87.60366291896175  \n3   41.78472618578524,-87.58821677767634  \n4     41.79955044222366,-87.593062823983  "
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Data is sourced from a personal project of mine you can find here:\n",
        "# https://ucpd-incident-reporter-7cfdc3369124.herokuapp.com/\n",
        "ucpd_reports = pd.read_csv(\"data/incident_dump.csv\")\n",
        "ucpd_reports.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7a4d052572243dfd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:53:55.117247Z",
          "start_time": "2024-02-05T00:52:59.573056Z"
        },
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/michaelp/Documents/GitHub/computational-content-analysis/.venv/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "ucpd_reports[\"tokenized_text\"] = ucpd_reports[\"comments\"].apply(\n",
        "    lucem_illud.word_tokenize\n",
        ")\n",
        "ucpd_reports[\"normalized_text\"] = ucpd_reports[\"tokenized_text\"].apply(\n",
        "    lucem_illud.normalizeTokens\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df8f5bc61d3bc989",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 1*</font>\n",
        "\n",
        "<font color=\"red\">Perform a content annotation survey of some kind in which at \n",
        "least 3 people evaluate and code each piece of content, using Amazon Mechanical \n",
        "Turk as described in the [MTurk slides on Canvas](https://canvas.uchicago.edu/courses/54694/files/folder/unfiled?preview=10675152), or by hand with friends.  \n",
        "With the resulting data, calculate, visualize and discuss inter-coder agreement or \n",
        "co-variation with appropriate metrics. What does this means for the reliability of \n",
        "human assessments regarding content in your domain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b28c30789992721a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:45:10.724873Z",
          "start_time": "2024-02-05T00:45:10.350705Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Figure out a way to chunk up the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4b7bd66b3e1bcf",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 2*</font>\n",
        "\n",
        "<font color=\"red\">Go back through all the cells above and generate 10 distinct \n",
        "artificial datasets and classify them with all the available methods. Add a cell \n",
        "immediately below and describe which classifier(s) worked best with which \n",
        "artificially constructed data source and why. Then go through all the empirical \n",
        "datasets (i.e., Newsgroups, Senate Small, Senate Large, Email Spam) and classify \n",
        "them with all available methods. Add a second cell immediately below and describe \n",
        "which classifier(s) worked best with which data set and why.\n",
        "\n",
        "<font color=\"red\">***Stretch*** (but also required) Wander through the SKLearn \n",
        "documentation available [here](http://scikit-learn.org/stable/), particularly \n",
        "perusing the classifiers. In cells following, identify and implement a new classifier \n",
        "that we have not yet used (e.g., AdaBoost, CART) on one artificial dataset and one real \n",
        "dataset (used above). Then, in the next cell describe the classifier, detail how it \n",
        "compares with the approaches above, and why it performed better or worse than others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6c838416f9e1cb55",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:45:10.724926Z",
          "start_time": "2024-02-05T00:45:10.352847Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7492cdac200a8ee4",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 3*</font>\n",
        "\n",
        "<font color=\"red\">In the cells immediately following, perform logistic regression \n",
        "classification using training, testing and un-coded (i.e., data you didn't code by \n",
        "hand but want to use your model on) data from texts and hand-classifications \n",
        "associated with your final project (e.g., these could be crowd-sourced codes \n",
        "gathered through Amazon Mechanical Turk in Exercise 1). Visualize the confusion \n",
        "matrix for training and testing sets. Calculate precision, recall, the F-measure, \n",
        "and AUC, then perform an ROC visualization. How do these classifiers perform? \n",
        "Extrapolate code from these models to all un-coded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6a9118ccc9481a85",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:45:10.724994Z",
          "start_time": "2024-02-05T00:45:10.354499Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "97dc2de83ad6231f",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 4*</font>\n",
        "\n",
        "<font color=\"red\">In the cells immediately following, perform decision tree and \n",
        "random forest classification (binary, multinomial or continuous) using training, \n",
        "testing and extrapolation (un-coded) data from texts and hand-classifications \n",
        "associated with your final project. As with ***Exercise 2***, these could be \n",
        "crowdsourced codes gathered through Amazon Mechanical Turk last week. Visualize \n",
        "the classification of data points. Calculate relevant metrics (e.g., precision, \n",
        "recall, the F-measure, and AUC). Now build an ensemble classifier by bagging trees \n",
        "into a random forest. Visualize the result. How do these classifiers perform? \n",
        "What does ensemble learning do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "be2e65f945decf2c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:45:10.732368Z",
          "start_time": "2024-02-05T00:45:10.356217Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7b459f9a4f8cef61",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 6*</font>\n",
        "\n",
        "<font color=\"red\">In the cells immediately following, perform a neural network \n",
        "classification and calculate relevant metrics (e.g., precision, recall, the \n",
        "F-measure, and AUC). How does this classify relevant to *k*-nearest neighbor, \n",
        "logistic and decision-tree approaches?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "af75b7d8174467e2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:45:10.732458Z",
          "start_time": "2024-02-05T00:45:10.358328Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d2796a48dd01a6f9",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "## <font color=\"red\">*Exercise 7*</font>\n",
        "\n",
        "<font color=\"red\">In the cells immediately following, use the pipeline functions \n",
        "or the word or sentence vector functions (e.g., similarity) to explore the social \n",
        "game underlying the production and meaning of texts associated with your final project. \n",
        "How does BERT help you gain insight regarding your research question that is similar \n",
        "and different from prior methods?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "98b1f3a988c7ca24",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-05T00:45:10.732505Z",
          "start_time": "2024-02-05T00:45:10.360204Z"
        },
        "collapsed": false
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f83e88d8954fa9eb",
      "metadata": {
        "collapsed": false
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
